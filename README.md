
# ğŸ§  Vision Transformer (ViT) on Fashion-MNIST  
**Author:** Pawan Sharma  
**Roll Number:** 22053329  
**Dataset:** Fashion-MNIST  
**Framework:** PyTorch  

---

## ğŸ“˜ Overview  
This project implements a **Mini Vision Transformer (ViT)** from scratch â€” without using any pretrained models â€” as part of the â€œTransformers Assignmentâ€.  
The model is trained on the **Fashion-MNIST** dataset to classify different clothing categories such as shirts, sneakers, trousers, and more.  

The implementation covers:
- Manual patch embedding and positional encoding  
- Multi-head self-attention and MLP blocks  
- Layer normalization and residual connections  
- End-to-end training and evaluation pipeline  

---

## âš™ï¸ Roll-Number Based Hyperparameters  
| Parameter | Formula | Value (for Roll 22053329) |
|------------|----------|---------------------------|
| `seed` | last two digits | **29** |
| `hidden_dim` | 128 + (29 % 5) * 32 | **256** |
| `num_heads` | 4 + (29 % 3) | **6** |
| `patch_size` | 8 + (29 % 4) * 2 | **10** |
| `epochs` | 10 + (29 % 5) | **14** |

---

## ğŸ§© Project Structure  
```
VisionTransformer-Assignment/
â”œâ”€â”€ vit_fashionmnist_partB.py         # Model training and implementation
â”œâ”€â”€ vit_fashionmnist_partC_fixed.py   # Evaluation, plots, attention visualization
â”œâ”€â”€ outputs_partB/
â”‚   â”œâ”€â”€ model_best.pt
â”‚   â””â”€â”€ logs.csv
â”œâ”€â”€ outputs_partC_fixed/
â”‚   â”œâ”€â”€ accuracy_curve.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ attention_map.png
â”‚   â””â”€â”€ report.txt
â””â”€â”€ Vision_Transformer_Assignment_Report_Pawan_Sharma.docx
```

---

## ğŸ§  Model Summary  
The Mini ViT model was trained for 14 epochs using the **AdamW optimizer** with cosine learning rate scheduling.  
It achieved a **final test accuracy of 89.92%**, demonstrating that even a compact ViT architecture can perform competitively with CNNs on small-scale vision tasks.

---

## ğŸ“Š Results and Visualizations  
- **Accuracy Curve:** Training and validation accuracies increased smoothly with no signs of overfitting.  
- **Confusion Matrix:** Clear diagonal dominance with minimal misclassifications.  
- **Attention Map:** Model focuses on key object regions (edges, collars, soles), showing interpretable spatial attention.  

<p align="center">
  <img src="outputs_partC_fixed/accuracy_curve.png" width="300">
  <img src="outputs_partC_fixed/confusion_matrix.png" width="300">
  <img src="outputs_partC_fixed/attention_map.png" width="300">
</p>

---

## ğŸ§¾ Conclusion  
The Mini Vision Transformer successfully learns discriminative representations from scratch.  
Its multi-head attention mechanism enables region-specific feature extraction, and the chosen patch size ensures a strong balance between local texture detail and efficiency.  
This demonstrates that ViTs, even at small scale, can achieve **CNN-level performance** when properly tuned.

---

## ğŸ§° Requirements  
Install dependencies:
```bash
pip install torch torchvision matplotlib seaborn scikit-learn
```

Run training:
```bash
python vit_fashionmnist_partB.py
```

Run evaluation and analysis:
```bash
python vit_fashionmnist_partC_fixed.py
```

---

## ğŸ Acknowledgements  
- Dataset: [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)  
- Architecture inspired by *â€œAn Image is Worth 16x16 Wordsâ€ (Dosovitskiy et al., 2020)*  
- Implementation and analysis authored by **Pawan Sharma**
